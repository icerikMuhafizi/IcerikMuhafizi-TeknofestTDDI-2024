# -*- coding: utf-8 -*-
"""modelKullanım.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRQnnT-YJbs1SE0Jw25XPe79ZJK-uSi5
"""

#!pip install google-api-python-client youtube-transcript-api pandas

import sys

from tqdm import tk
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
import pandas as pd
import re
from urllib.parse import urlparse, parse_qs
import torch

model_name = "Zuhall/bert-base-turkish-uncased-IM"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# API anahtarınızı buraya girin
api_key = 'AIzaSyDQA8HEXmUPXNS_jtlitaPy9t68XZKtuOA'

# YouTube API istemcisi oluşturun
youtube = build('youtube', 'v3', developerKey=api_key)


url = sys.argv[1]  # main.py dosyasından URL'yi alır

def get_video_id(url):
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)

    return query_params.get('v', [None])[0]

video_id = get_video_id(url)

request = youtube.videos().list(
    part="snippet",
    id=video_id
)
response = request.execute()

# Türkçe transkripti al
transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['tr'])
formatter = TextFormatter()
transcript = formatter.format_transcript(transcript_list)

# Veriyi bir DataFrame'e koy
data = {
    'transcript': transcript
}
df = pd.DataFrame([data])

def split_by_capital(text):
    # Büyük harfle başlayan yerlerde bölme
    # Büyük harfle başlayan kelimeleri bul
    pattern = r'(?<!^)(?<!\.\s)(?<!\!\s)(?<!\?\s)(?<!\.\s)(?<!\!\s)(?<!\?\s)(?<!\!\s)(?<!\?\s)[A-Z]'

    # Büyük harfle başlayan yerlerde böl
    sentences = re.split(pattern, text)

    # Büyük harfle başlayan kelimeleri listele
    capitalized_words = re.findall(pattern, text)

    # Kapalı cümleler için
    capitalized_sentences = []
    index = 0
    first_sentence = sentences[0].strip()
    capitalized_sentences.append(first_sentence)


    for word in capitalized_words:
        if index < len(sentences):
            index += 1
            sentence = sentences[index]
            capitalized_sentences.append(word + sentence)

    # Boşlukları temizle
    capitalized_sentences = [s.strip() for s in capitalized_sentences if s.strip()]

    return capitalized_sentences


text = ' '.join(df['transcript'].astype(str))
text = text.replace('\n', ' ')  # \n karakterlerini boşluk ile değiştirme

sentences = split_by_capital(text)

df = pd.DataFrame(sentences, columns=['sentence'])


# Temizlik ve Normalizasyon
def preprocess_text(text):
    if isinstance(text, float):
        text = ""
    text = re.sub(r'\W', ' ', text)
    # Sayıları temizleme
    text = re.sub(r'\d', ' ', text)
    # Çoklu boşlukları tek boşlukla değiştirme
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['sentence'] = df['sentence'].apply(preprocess_text)

df.to_csv('gezgin_celebi.csv', index=False)

def onislem(text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    return predictions.item()

sonuclar = []
for index, row in df.iterrows():
    sonuc = onislem(row['sentence'])
    sonuclar.append(sonuc)

df['label'] = sonuclar
a= df['label'].value_counts()

# Sonuçları yorumla
labels = {0: "notr", 1: "olumlu", 2: "olumsuz", 3: "olumsuz"} # Label yorumlarını burada belirtiyoruz
most_common_label = df['label'].mode()[0]
result = labels.get(most_common_label, "belirsiz")

def label_counts(df):
    counts = df['label'].value_counts().to_dict()

    label_0_count = counts.get(0, 0)
    label_1_count = counts.get(1, 0)
    label_2_count = counts.get(2, 0)
    label_3_count = counts.get(3, 0)
    label_4_count = counts.get(4, 0)
    label_5_count = counts.get(5, 0)
    label_6_count = counts.get(6, 0)

    return {
        "0 label": label_0_count,
        "1 label": label_1_count,
        "2 label": label_2_count,
        "3 label": label_3_count,
        "4 label": label_4_count,
        "5 label": label_5_count,
        "6 label": label_6_count,
    }

etiket_sayilari= label_counts(df)

toplamYuzde = 0.1 * (
    int(etiket_sayilari["0 label"]) +
    int(etiket_sayilari["1 label"]) +
    int(etiket_sayilari["2 label"]) +
    int(etiket_sayilari["3 label"]) +
    int(etiket_sayilari["4 label"]) +
    int(etiket_sayilari["5 label"]) +
    int(etiket_sayilari["6 label"])
)

toplam456 = (
    int(etiket_sayilari["4 label"]) +
    int(etiket_sayilari["5 label"]) +
    int(etiket_sayilari["6 label"])
)

if toplam456 > toplamYuzde:
    label_names = {4: "Argo/Kufur icerik", 5: "Mustehcen icerik", 6: "Siddet/Korku icerik"}
    uygunsuz_etiketler = {k: etiket_sayilari.get(f"{k} label", 0) for k in [4, 5, 6]}

    max_label = max(uygunsuz_etiketler, key=uygunsuz_etiketler.get)
    max_label_name = label_names.get(max_label, "Bilinmeyen Etiket")

    print("Bu icerik cocuklar icin uygunsuz.")
    print("Uygunsuzluk Nedeni: ", max_label_name)

elif etiket_sayilari["0 label"] > 9 * toplamYuzde:
    print("Bu video %90 oranından fazla notr icerige sahiptir.")
else:
    label_names2 = {1: "Eglence/Macera icerik", 2: "Egitici icerik", 3: "Arkadas canlisi icerik"}

    uygun_etiketler = {k: etiket_sayilari.get(f"{k} label", 0) for k in [1, 2, 3]}

    # En yüksek etiket sayısını ve ismini bulmak
    max_label2 = max(uygun_etiketler, key=uygun_etiketler.get)
    max_label_name2 = label_names2.get(max_label2, "Bilinmeyen Etiket")

    print("Bu icerik cocuklar icin uygundur.")
    print("Uygunluk Nedeni: ", max_label_name2)
